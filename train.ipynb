{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289},{"sourceId":347412,"sourceType":"modelInstanceVersion","modelInstanceId":290157,"modelId":310880},{"sourceId":347413,"sourceType":"modelInstanceVersion","modelInstanceId":290158,"modelId":310881},{"sourceId":347414,"sourceType":"modelInstanceVersion","modelInstanceId":290159,"modelId":310882},{"sourceId":347416,"sourceType":"modelInstanceVersion","modelInstanceId":290161,"modelId":310884},{"sourceId":347419,"sourceType":"modelInstanceVersion","modelInstanceId":290164,"modelId":310886}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data.dataset import Dataset\nfrom torch.utils.data import DataLoader\nfrom torchvision import models, transforms\nimport json\nfrom PIL import Image\nimport torch.nn.functional as F\nimport math\nimport os\nimport re\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:13:50.020706Z","iopub.execute_input":"2025-04-21T08:13:50.021331Z","iopub.status.idle":"2025-04-21T08:14:00.068518Z","shell.execute_reply.started":"2025-04-21T08:13:50.021298Z","shell.execute_reply":"2025-04-21T08:14:00.067761Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"IMAGES_FOLDER = '/kaggle/input/flickr8k/Images'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:14:00.069839Z","iopub.execute_input":"2025-04-21T08:14:00.070184Z","iopub.status.idle":"2025-04-21T08:14:00.073569Z","shell.execute_reply.started":"2025-04-21T08:14:00.070157Z","shell.execute_reply":"2025-04-21T08:14:00.072808Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"w2i = torch.load(\"/kaggle/input/w2i/pytorch/default/1/w2i.pt\")\nval_caption = torch.load(\"/kaggle/input/val_captions/pytorch/default/1/val_captions.pt\")\ntrain_caption = torch.load(\"/kaggle/input/train_captions/pytorch/default/1/train_captions.pt\")\ni2w = torch.load(\"/kaggle/input/i2w/pytorch/default/1/i2w.pt\")\nembedding_matrix = torch.load(\"/kaggle/input/embedding_matrix/pytorch/default/1/embedding_matrix.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:14:00.074182Z","iopub.execute_input":"2025-04-21T08:14:00.074481Z","iopub.status.idle":"2025-04-21T08:14:00.396872Z","shell.execute_reply.started":"2025-04-21T08:14:00.074456Z","shell.execute_reply":"2025-04-21T08:14:00.396258Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"==================================================================================================\nCREATE LOADER\n===================================================================================================\n","metadata":{}},{"cell_type":"code","source":"w2i = {word: idx - 1 for word, idx in w2i.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:14:00.398190Z","iopub.execute_input":"2025-04-21T08:14:00.398416Z","iopub.status.idle":"2025-04-21T08:14:00.402187Z","shell.execute_reply.started":"2025-04-21T08:14:00.398399Z","shell.execute_reply":"2025-04-21T08:14:00.401669Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"i2w = {idx: word for word, idx in w2i.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:14:00.402912Z","iopub.execute_input":"2025-04-21T08:14:00.403789Z","iopub.status.idle":"2025-04-21T08:14:00.416039Z","shell.execute_reply.started":"2025-04-21T08:14:00.403768Z","shell.execute_reply":"2025-04-21T08:14:00.415400Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def convert_sentences_to_index(caption):\n    for cap in caption:\n        encoded_caption = [w2i[word] for word in cap.split(' ') if word in w2i]\n    return encoded_caption\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:14:00.416827Z","iopub.execute_input":"2025-04-21T08:14:00.416991Z","iopub.status.idle":"2025-04-21T08:14:00.428598Z","shell.execute_reply.started":"2025-04-21T08:14:00.416976Z","shell.execute_reply":"2025-04-21T08:14:00.428006Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_image_caption_pair(dataset, image_path):\n    image_to_caption = []\n    for image, captions in dataset.items():\n        image = image_path + '/' + image\n        encoded_caption = convert_sentences_to_index(captions)\n        image_to_caption.append([image, encoded_caption])\n    return image_to_caption","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:14:00.429237Z","iopub.execute_input":"2025-04-21T08:14:00.429459Z","iopub.status.idle":"2025-04-21T08:14:00.442849Z","shell.execute_reply.started":"2025-04-21T08:14:00.429436Z","shell.execute_reply":"2025-04-21T08:14:00.442128Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_max_len(data_set):\n    max_len = 0\n    for data in data_set:\n        max_len = max(max_len, len(data[1]))\n        \n    return max_len","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:14:00.443751Z","iopub.execute_input":"2025-04-21T08:14:00.443998Z","iopub.status.idle":"2025-04-21T08:14:00.455219Z","shell.execute_reply.started":"2025-04-21T08:14:00.443977Z","shell.execute_reply":"2025-04-21T08:14:00.454643Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FlickrDataset(Dataset):\n    def __init__(self, data, training=True):\n        self.transform = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor()\n        ])\n        self.training = training\n        self.max_len = get_max_len(data)\n        self.samples = []\n        for sample in data:\n            image = sample[0]\n            caption = sample[1]\n            if training:\n                for i in range(1, len(caption)):\n                    in_seq, out_seq = caption[:i], caption[i]\n                    self.samples.append((image, in_seq, out_seq))\n            else:\n                self.samples.append((image, caption))\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        if self.training:\n            img_path, in_seq, out_token = self.samples[idx]\n        else:\n            img_path, caption = self.samples[idx]\n    \n        image = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n    \n        if self.training:\n            in_seq = torch.tensor(in_seq)\n            padded_seq = torch.zeros(self.max_len, dtype=torch.long)\n            length = min(len(in_seq), self.max_len)\n            padded_seq[-length:] = in_seq[-length:]\n    \n            out_token = torch.tensor(out_token)\n            return image, padded_seq, out_token\n        else:\n            return image, caption","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:32:53.741501Z","iopub.execute_input":"2025-04-21T08:32:53.742217Z","iopub.status.idle":"2025-04-21T08:32:53.749108Z","shell.execute_reply.started":"2025-04-21T08:32:53.742182Z","shell.execute_reply":"2025-04-21T08:32:53.748279Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_dataset(training_set, validation_set, image_path):\n    train_set = create_image_caption_pair(training_set, image_path)\n    val_set = create_image_caption_pair(validation_set, image_path)\n    \n    train_data = FlickrDataset(train_set)\n    val_data = FlickrDataset(val_set, training = False)\n    train_loader = DataLoader(train_data, batch_size = 16, shuffle = False)\n    val_loader = DataLoader(val_data, batch_size = 1, shuffle = False)\n    return train_loader, val_loader\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:32:56.073835Z","iopub.execute_input":"2025-04-21T08:32:56.074699Z","iopub.status.idle":"2025-04-21T08:32:56.079145Z","shell.execute_reply.started":"2025-04-21T08:32:56.074671Z","shell.execute_reply":"2025-04-21T08:32:56.078397Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader, val_loader = get_dataset(train_caption, val_caption, IMAGES_FOLDER)\n\nprint(len(train_loader), len(val_loader))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:32:57.943666Z","iopub.execute_input":"2025-04-21T08:32:57.943944Z","iopub.status.idle":"2025-04-21T08:32:58.390171Z","shell.execute_reply.started":"2025-04-21T08:32:57.943926Z","shell.execute_reply":"2025-04-21T08:32:58.389513Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"==================================================================================================\nCREATE MODEL\n===================================================================================================\n","metadata":{}},{"cell_type":"code","source":"class VGG(nn.Module):\n    def __init__(self):\n        super(VGG, self).__init__()\n        vgg              = models.vgg16(pretrained=True)\n        self.features    = vgg.features\n        self.avgpooling  = vgg.avgpool\n        self.flatten     = nn.Flatten()\n\n        for param in self.features.parameters():\n            param.requires_grad = False\n    def forward(self, x):\n        out = self.features(x)\n        out = self.avgpooling(out)\n        out = self.flatten(out)\n\n        return out\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:14:01.032682Z","iopub.execute_input":"2025-04-21T08:14:01.032928Z","iopub.status.idle":"2025-04-21T08:14:01.037703Z","shell.execute_reply.started":"2025-04-21T08:14:01.032910Z","shell.execute_reply":"2025-04-21T08:14:01.036867Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class KANLinear(nn.Module):\n    def __init__(\n        self,\n        in_features,\n        out_features,\n        grid_size=5,\n        spline_order=3,\n        scale_noise=0.1,\n        scale_base=1.0,\n        scale_spline=1.0,\n        enable_standalone_scale_spline=True,\n        base_activation=nn.SiLU,\n        grid_eps=0.02,\n        grid_range=[-1, 1],\n    ):\n        super(KANLinear, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.grid_size = grid_size\n        self.spline_order = spline_order\n\n        h = (grid_range[1] - grid_range[0]) / grid_size\n        grid = (\n            (\n                torch.arange(-spline_order, grid_size + spline_order + 1) * h\n                + grid_range[0]\n            )\n            .expand(in_features, -1)\n            .contiguous()\n        )\n        self.register_buffer(\"grid\", grid)\n\n        self.base_weight = nn.Parameter(torch.Tensor(out_features, in_features))\n        self.spline_weight = nn.Parameter(\n            torch.Tensor(out_features, in_features, grid_size + spline_order)\n        )\n        if enable_standalone_scale_spline:\n            self.spline_scaler = nn.Parameter(\n                torch.Tensor(out_features, in_features)\n            )\n\n        self.scale_noise = scale_noise\n        self.scale_base = scale_base\n        self.scale_spline = scale_spline\n        self.enable_standalone_scale_spline = enable_standalone_scale_spline\n        self.base_activation = base_activation()\n        self.grid_eps = grid_eps\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.kaiming_uniform_(self.base_weight, a=math.sqrt(5) * self.scale_base)\n        with torch.no_grad():\n            noise = (\n                (\n                    torch.rand(self.grid_size + 1, self.in_features, self.out_features)\n                    - 1 / 2\n                )\n                * self.scale_noise\n                / self.grid_size\n            )\n            self.spline_weight.data.copy_(\n                (self.scale_spline if not self.enable_standalone_scale_spline else 1.0)\n                * self.curve2coeff(\n                    self.grid.T[self.spline_order : -self.spline_order],\n                    noise,\n                )\n            )\n            if self.enable_standalone_scale_spline:\n                # torch.nn.init.constant_(self.spline_scaler, self.scale_spline)\n                nn.init.kaiming_uniform_(self.spline_scaler, a=math.sqrt(5) * self.scale_spline)\n\n    def b_splines(self, x: torch.Tensor):\n        \"\"\"\n        Compute the B-spline bases for the given input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n\n        Returns:\n            torch.Tensor: B-spline bases tensor of shape (batch_size, in_features, grid_size + spline_order).\n        \"\"\"\n        assert x.dim() == 2 and x.size(1) == self.in_features\n\n        grid: torch.Tensor = (\n            self.grid\n        )  # (in_features, grid_size + 2 * spline_order + 1)\n        x = x.unsqueeze(-1)\n        bases = ((x >= grid[:, :-1]) & (x < grid[:, 1:])).to(x.dtype)\n        for k in range(1, self.spline_order + 1):\n            bases = (\n                (x - grid[:, : -(k + 1)])\n                / (grid[:, k:-1] - grid[:, : -(k + 1)])\n                * bases[:, :, :-1]\n            ) + (\n                (grid[:, k + 1 :] - x)\n                / (grid[:, k + 1 :] - grid[:, 1:(-k)])\n                * bases[:, :, 1:]\n            )\n\n        assert bases.size() == (\n            x.size(0),\n            self.in_features,\n            self.grid_size + self.spline_order,\n        )\n        return bases.contiguous()\n\n    def curve2coeff(self, x: torch.Tensor, y: torch.Tensor):\n        \"\"\"\n        Compute the coefficients of the curve that interpolates the given points.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n            y (torch.Tensor): Output tensor of shape (batch_size, in_features, out_features).\n\n        Returns:\n            torch.Tensor: Coefficients tensor of shape (out_features, in_features, grid_size + spline_order).\n        \"\"\"\n        assert x.dim() == 2 and x.size(1) == self.in_features\n        assert y.size() == (x.size(0), self.in_features, self.out_features)\n\n        A = self.b_splines(x).transpose(\n            0, 1\n        )  # (in_features, batch_size, grid_size + spline_order)\n        B = y.transpose(0, 1)  # (in_features, batch_size, out_features)\n        solution = torch.linalg.lstsq(\n            A, B\n        ).solution  # (in_features, grid_size + spline_order, out_features)\n        result = solution.permute(\n            2, 0, 1\n        )  # (out_features, in_features, grid_size + spline_order)\n\n        assert result.size() == (\n            self.out_features,\n            self.in_features,\n            self.grid_size + self.spline_order,\n        )\n        return result.contiguous()\n\n    @property\n    def scaled_spline_weight(self):\n        return self.spline_weight * (\n            self.spline_scaler.unsqueeze(-1)\n            if self.enable_standalone_scale_spline\n            else 1.0\n        )\n\n    def forward(self, x: torch.Tensor):\n        assert x.size(-1) == self.in_features\n        original_shape = x.shape\n        x = x.reshape(-1, self.in_features)\n\n        base_output = F.linear(self.base_activation(x), self.base_weight)\n        spline_output = F.linear(\n            self.b_splines(x).view(x.size(0), -1),\n            self.scaled_spline_weight.view(self.out_features, -1),\n        )\n        output = base_output + spline_output\n        \n        output = output.reshape(*original_shape[:-1], self.out_features)\n        return output\n\n    @torch.no_grad()\n    def update_grid(self, x: torch.Tensor, margin=0.01):\n        assert x.dim() == 2 and x.size(1) == self.in_features\n        batch = x.size(0)\n\n        splines = self.b_splines(x)  # (batch, in, coeff)\n        splines = splines.permute(1, 0, 2)  # (in, batch, coeff)\n        orig_coeff = self.scaled_spline_weight  # (out, in, coeff)\n        orig_coeff = orig_coeff.permute(1, 2, 0)  # (in, coeff, out)\n        unreduced_spline_output = torch.bmm(splines, orig_coeff)  # (in, batch, out)\n        unreduced_spline_output = unreduced_spline_output.permute(\n            1, 0, 2\n        )  # (batch, in, out)\n\n        # sort each channel individually to collect data distribution\n        x_sorted = torch.sort(x, dim=0)[0]\n        grid_adaptive = x_sorted[\n            torch.linspace(\n                0, batch - 1, self.grid_size + 1, dtype=torch.int64, device=x.device\n            )\n        ]\n\n        uniform_step = (x_sorted[-1] - x_sorted[0] + 2 * margin) / self.grid_size\n        grid_uniform = (\n            torch.arange(\n                self.grid_size + 1, dtype=torch.float32, device=x.device\n            ).unsqueeze(1)\n            * uniform_step\n            + x_sorted[0]\n            - margin\n        )\n\n        grid = self.grid_eps * grid_uniform + (1 - self.grid_eps) * grid_adaptive\n        grid = torch.concatenate(\n            [\n                grid[:1]\n                - uniform_step\n                * torch.arange(self.spline_order, 0, -1, device=x.device).unsqueeze(1),\n                grid,\n                grid[-1:]\n                + uniform_step\n                * torch.arange(1, self.spline_order + 1, device=x.device).unsqueeze(1),\n            ],\n            dim=0,\n        )\n\n        self.grid.copy_(grid.T)\n        self.spline_weight.data.copy_(self.curve2coeff(x, unreduced_spline_output))\n\n    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n        \"\"\"\n        Compute the regularization loss.\n\n        This is a dumb simulation of the original L1 regularization as stated in the\n        paper, since the original one requires computing absolutes and entropy from the\n        expanded (batch, in_features, out_features) intermediate tensor, which is hidden\n        behind the F.linear function if we want an memory efficient implementation.\n\n        The L1 regularization is now computed as mean absolute value of the spline\n        weights. The authors implementation also includes this term in addition to the\n        sample-based regularization.\n        \"\"\"\n        l1_fake = self.spline_weight.abs().mean(-1)\n        regularization_loss_activation = l1_fake.sum()\n        p = l1_fake / regularization_loss_activation\n        regularization_loss_entropy = -torch.sum(p * p.log())\n        return (\n            regularize_activation * regularization_loss_activation\n            + regularize_entropy * regularization_loss_entropy\n        )\n\n\nclass KAN(nn.Module):\n    def __init__(\n        self,\n        layers_hidden,\n        grid_size=5,\n        spline_order=3,\n        scale_noise=0.1,\n        scale_base=1.0,\n        scale_spline=1.0,\n        base_activation=nn.SiLU,\n        grid_eps=0.02,\n        grid_range=[-1, 1],\n    ):\n        super(KAN, self).__init__()\n        self.grid_size = grid_size\n        self.spline_order = spline_order\n\n        self.layers = nn.ModuleList()\n        for in_features, out_features in zip(layers_hidden, layers_hidden[1:]):\n            self.layers.append(\n                KANLinear(\n                    in_features,\n                    out_features,\n                    grid_size=grid_size,\n                    spline_order=spline_order,\n                    scale_noise=scale_noise,\n                    scale_base=scale_base,\n                    scale_spline=scale_spline,\n                    base_activation=base_activation,\n                    grid_eps=grid_eps,\n                    grid_range=grid_range,\n                )\n            )\n\n    def forward(self, x: torch.Tensor, update_grid=False):\n        for layer in self.layers:\n            if update_grid:\n                layer.update_grid(x)\n            x = layer(x)\n        return x\n\n    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n        return sum(\n            layer.regularization_loss(regularize_activation, regularize_entropy)\n            for layer in self.layers\n        )","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-04-21T08:14:01.038539Z","iopub.execute_input":"2025-04-21T08:14:01.038982Z","iopub.status.idle":"2025-04-21T08:14:01.070690Z","shell.execute_reply.started":"2025-04-21T08:14:01.038960Z","shell.execute_reply":"2025-04-21T08:14:01.070153Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:14:01.071295Z","iopub.execute_input":"2025-04-21T08:14:01.071501Z","iopub.status.idle":"2025-04-21T08:14:01.111039Z","shell.execute_reply.started":"2025-04-21T08:14:01.071479Z","shell.execute_reply":"2025-04-21T08:14:01.110541Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class VGGLSTM(nn.Module):\n    def __init__(self, input_feature_size, vocab_size, embed_size, hidden_size):\n        super(VGGLSTM, self).__init__()\n        self.vgg = VGG()\n        self.feature_projector = nn.Linear(input_feature_size, embed_size)\n        self.embed = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, vocab_size)\n\n    def forward(self, image, captions):\n        features = self.vgg(image)\n        projected_features = self.feature_projector(features)  # [batch, embed_size]\n        projected_features = projected_features.unsqueeze(1)  # [batch, 1, embed_size]\n\n        captions_embed = self.embed(captions)  # [batch, seq_len, embed_size]\n        lstm_input = torch.cat((projected_features, captions_embed), dim=1)\n\n        lstm_out, _ = self.lstm(lstm_input)\n        out = self.fc(lstm_out[:, -1, :])  # Output from the last time step\n        return out\nmodel = VGGLSTM(\n    input_feature_size=512 * 7 * 7,\n    vocab_size=len(w2i),\n    embed_size=200,\n    hidden_size=512\n)\n\n# class VGGKANLSTM(nn.Module):\n#     def __init__(self, layers_hidden, vocab_size, embed_size, hidden_size, grid_size=5, spline_order=3):\n#         super(VGGKANLSTM, self).__init__()\n#         self.vgg = VGG()  # VGG encoder\n#         self.kan = KAN(layers_hidden, grid_size=grid_size, spline_order=spline_order)  # KAN layer\n#         self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)  # LSTM for sequence generation\n#         self.fc = nn.Linear(hidden_size, vocab_size)  # Output layer for word prediction\n#         self.feature_projector = nn.Linear(layers_hidden[-1], embed_size)\n#         self.embed = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n        \n#     def forward(self, image, captions):\n\n#         features = self.vgg(image) \n#         # print(f\"after vgg: {features.shape}\")\n#         refined_features = self.kan(features) \n#         # print(f\"after kan: {refined_features.shape}\")\n\n#         refined_features = self.feature_projector(refined_features)  # [1, 256]\n#         refined_features = refined_features.unsqueeze(1)  # [1, 1, 256]\n\n#         captions_embed = self.embed(captions) \n#         # print(f\"captions embed: {captions_embed.shape}\")\n\n#         lstm_input = torch.cat((refined_features, captions_embed), dim=1)\n#         # print(f\"lstm input: {lstm_input.shape}\")\n\n#         lstm_out, (hn, cn) = self.lstm(lstm_input)  \n\n#         out = self.fc(lstm_out[:, -1, :])\n        \n#         return out\n\n\n# Example usage\n# model = VGGLSTM(layers_hidden=[512 * 7 * 7, 1024, 512], vocab_size=len(w2i), embed_size=200, hidden_size=512)\nmodel.to(\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:14:01.111658Z","iopub.execute_input":"2025-04-21T08:14:01.111885Z","iopub.status.idle":"2025-04-21T08:14:15.257585Z","shell.execute_reply.started":"2025-04-21T08:14:01.111866Z","shell.execute_reply":"2025-04-21T08:14:15.256842Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Number of network parameters:', sum(param.numel() for param in model.parameters()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:14:15.259544Z","iopub.execute_input":"2025-04-21T08:14:15.259771Z","iopub.status.idle":"2025-04-21T08:14:15.264473Z","shell.execute_reply.started":"2025-04-21T08:14:15.259754Z","shell.execute_reply":"2025-04-21T08:14:15.263522Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb.login(key = \"5dd930565a80444a1b9c4c6613a2c773637a4b4c\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T16:02:10.198131Z","iopub.execute_input":"2025-04-18T16:02:10.198936Z","iopub.status.idle":"2025-04-18T16:02:17.567839Z","shell.execute_reply.started":"2025-04-18T16:02:10.198909Z","shell.execute_reply":"2025-04-18T16:02:17.567082Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"==================================================================================================\nCREATE LOSS FUNCTION\n===================================================================================================\n","metadata":{}},{"cell_type":"code","source":"learning_rate = 1e-3\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T16:02:17.568617Z","iopub.execute_input":"2025-04-18T16:02:17.569493Z","iopub.status.idle":"2025-04-18T16:02:17.573439Z","shell.execute_reply.started":"2025-04-18T16:02:17.569464Z","shell.execute_reply":"2025-04-18T16:02:17.572817Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"==================================================================================================\nCREATE TRAINER\n===================================================================================================\n","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport wandb\n\ndef trainer(model, train_loader, val_loader, optimizer, loss_func,\n            start_epoch, end_epoch, save_every=1, checkpoint_path=None,\n            save_path=\"model.pt\", wb_tracking=True):\n\n    if wb_tracking: \n        wandb.init(\n            entity=\"CS338\",\n            project=\"image_captioning\",\n            name=\"test\",\n            config={\n                \"epochs\": end_epoch, \n                \"optimizer\": optimizer.__class__.__name__,\n                \"learning_rate\": optimizer.param_groups[0][\"lr\"],\n                \"loss_func\": loss_func.__class__.__name__,\n            },\n            settings=wandb.Settings(init_timeout=30)\n        )\n\n    if checkpoint_path and os.path.exists(checkpoint_path):\n        checkpoint = torch.load(checkpoint_path)\n        model.load_state_dict(checkpoint[\"model_state_dict\"])\n        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n        start_epoch = checkpoint[\"epoch\"] + 1\n        print(f\"Resuming training from epoch {start_epoch}\")\n        \n    for epoch in range(start_epoch, end_epoch):\n        model.train()\n        epoch_loss = 0\n        print({f\"Start training epoch {epoch}\"})\n        for idx, batch_data in enumerate(train_loader):\n            image, pads, label = batch_data\n            image = image.to(device)\n            pads = pads.to(device)\n            label = label.to(device)\n    \n            optimizer.zero_grad()\n            outputs = model(image, pads)\n            loss = criterion(outputs, label)\n    \n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n            \n            if wb_tracking:\n                wandb.log({\"train_step_loss\": loss.item()})\n        \n        avg_loss = epoch_loss / len(train_loader)\n        if wb_tracking:\n            wandb.log({\n                \"train_epoch_loss\": avg_loss,\n                \"epoch\": epoch\n            })\n\n        if (epoch + 1) % save_every == 0 or epoch == end_epoch - 1:\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict()\n            }, save_path)\n            print(f\"Checkpoint saved at epoch {epoch} to {save_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T16:02:17.574214Z","iopub.execute_input":"2025-04-18T16:02:17.574472Z","iopub.status.idle":"2025-04-18T16:02:17.594237Z","shell.execute_reply.started":"2025-04-18T16:02:17.574450Z","shell.execute_reply":"2025-04-18T16:02:17.593500Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nsave_dir = 'kaggle/working'\nstart = 0\nend = 30\nbatch_size = 1\nsave_every = 1\ncheckpoint_path = \"\"\nos.makedirs(save_dir, exist_ok = True)\nsave_path = os.path.join(save_dir, \"best_metric_model.pth\")\nprint(len(train_loader))\ntrainer(model, train_loader, val_loader, optimizer, criterion, start, end, save_every, checkpoint_path, save_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T16:02:17.594991Z","iopub.execute_input":"2025-04-18T16:02:17.595240Z","execution_failed":"2025-04-18T16:02:47.376Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def generate_caption_beam_search(model, image, i2w, w2i, transform=None, beam_width=3, max_len=20, device=\"cuda\"):\n#     model.to(device)\n#     model.eval()\n\n#     with torch.no_grad():\n#         image_tensor = image.to(device)  # Không dùng transform\n#         features = model.vgg(image_tensor)\n#         refined_features = model.kan(features)\n#         refined_features = model.feature_projector(refined_features).unsqueeze(1)\n\n#         start_token = w2i[\"startseq\"]\n#         end_token = w2i[\"endseq\"]\n#         beam = [(0.0, [start_token], None)]\n\n#         for _ in range(max_len):\n#             new_beam = []\n#             for log_prob, caption_idx, hidden in beam:\n#                 if caption_idx[-1] == end_token:\n#                     new_beam.append((log_prob, caption_idx, hidden))\n#                     continue\n\n#                 input_seq = torch.tensor([caption_idx[1:]], dtype=torch.long).to(device) if len(caption_idx) > 1 else torch.tensor([[start_token]], dtype=torch.long).to(device)\n#                 caption_embed = model.embed(input_seq)\n#                 lstm_input = torch.cat((refined_features, caption_embed), dim=1)\n\n#                 lstm_out, hidden_out = model.lstm(lstm_input, hidden)\n#                 output = model.fc(lstm_out[:, -1, :])\n#                 log_probs = torch.nn.functional.log_softmax(output, dim=1).squeeze(0)\n\n#                 top_log_probs, top_indices = torch.topk(log_probs, beam_width)\n\n#                 for i in range(beam_width):\n#                     next_idx = top_indices[i].item()\n#                     total_log_prob = log_prob + top_log_probs[i].item()\n#                     new_caption_idx = caption_idx + [next_idx]\n#                     new_beam.append((total_log_prob, new_caption_idx, hidden_out))\n\n#             beam = sorted(new_beam, key=lambda x: x[0], reverse=True)[:beam_width]\n\n#         best_caption = beam[0][1]\n#         caption_words = []\n#         for idx in best_caption[1:]:\n#             word = i2w[idx]\n#             if word == \"endseq\":\n#                 break\n#             caption_words.append(word)\n\n#         return \" \".join(caption_words)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:44:33.425934Z","iopub.execute_input":"2025-04-21T08:44:33.426226Z","iopub.status.idle":"2025-04-21T08:44:33.434821Z","shell.execute_reply.started":"2025-04-21T08:44:33.426206Z","shell.execute_reply":"2025-04-21T08:44:33.433942Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from tqdm import tqdm\n# from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\n# def evaluate_bleu(model, val_data, i2w, w2i, transform, device=\"cuda\"):\n#     smoothie = SmoothingFunction().method4\n#     model.eval()\n#     scores = []\n\n#     for image_tensor, caption in tqdm(val_data, desc=\"Evaluating BLEU\"):\n#         image = image_tensor.unsqueeze(0).to(device)  # Thêm batch dimension\n#         reference = [[i2w[idx] for idx in caption[1:]]]  # Bỏ <start>\n#         prediction = generate_caption_beam_search(model, image, i2w, w2i, beam_width=3, device=device)\n#         candidate = prediction.split()\n\n#         bleu_score = sentence_bleu(reference, candidate, weights=(0.5, 0.5), smoothing_function=smoothie)\n#         scores.append(bleu_score)\n\n#     return sum(scores) / len(scores)\n\n\n\n# val_bleu = evaluate_bleu(model, val_data=val_loader.dataset, i2w=i2w, w2i=w2i, transform=val_loader.dataset.transform)\n# print(\"BLEU Score:\", val_bleu)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:44:34.221280Z","iopub.execute_input":"2025-04-21T08:44:34.222022Z","iopub.status.idle":"2025-04-21T08:50:12.029668Z","shell.execute_reply.started":"2025-04-21T08:44:34.221994Z","shell.execute_reply":"2025-04-21T08:50:12.028804Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}